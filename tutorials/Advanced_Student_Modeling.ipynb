{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10 - Advanced Knowledge Tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lecture 10 (Advanced Student Modelling), you become familiar with a range of extensions of a traditional BKT model, each devoted to solve one or more of its limitations. Specifically, the lecture covered four key strategies: Forgetting (BKT+F), Latent Factor and Knowledge Tracing (LFKT), Feature-Aware Student Knowledge (FAST), and Deep Knowledge Tracing (DKT). \n",
    "\n",
    "In this tutorial, you will get hands-on experience on the BKT+F and DKT strategies covered in class to face some of the limitations of the traditional BKT models and improve the model ability of correctly predicting students' knowledge, with a focus on results discuss and interpretation. \n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "- Evaluate BKT and BKT + Forgets on different skills and data sets.\n",
    "- Discuss learning curves for BKT and BKT + Forgets on skills.\n",
    "- Create and fit a deep knowledge tracing model. \n",
    "- Evaluate BKT and Deep BKT on different skills and data sets.\n",
    "- Run ablation studies on the hyper-parameter of Deep BKT. \n",
    "- Discuss learning curves for BKT and Deep KT on skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import json\n",
    "import math\n",
    "\n",
    "from pyBKT.models import Model\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Sets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Assistments\n",
    "\n",
    "[2009-2010 ASSISTment Data](https://sites.google.com/site/assistmentsdata/)\n",
    "\n",
    "Assistments is a data set of 4,217 middle-school students practicing an electronic tutor that teaches and evaluates students in grade-school math, with a total of 525,534 trials. The student data are in a comma-delimited text file with one row per trial. The columns should correspond to a trial's user id, the order id (timestamp), the skill name, and and whether the student produced a correct response in the trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['assistments'] = pd.read_csv('./data/assistments_small.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['assistments'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Cognitive Tutor\n",
    "\n",
    "[Stamper, J., Niculescu-Mizil, A., Ritter, S., Gordon, G.J., & Koedinger, K.R. (2010). Algebra I 2008-2009. Challenge data set from KDD Cup 2010 Educational Data Mining Challenge.](http://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp)\n",
    "\n",
    "Cognitive Tutor is a data set of students practicing algebra exercises, with a total of 16,857 trials. The student data are in a space-delimited text file with one row per trial. The student data are in a comma-delimited text file with one row per trial. The columns should correspond to a trial's user id, the order id (timestamp), the skill name, and and whether the student produced a correct response in the trial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['cognitivetutor'] = pd.read_csv('./data/cognitivetutor_small.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['cognitivetutor'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Set Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    lst.append([name, len(data['user_id'].unique()), len(data.index), len(data['skill_name'].unique())])\n",
    "    \n",
    "pd.DataFrame(lst, columns=['Dataset Name', 'Students', 'Observations', 'Skills'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparing BKT and BKT+F\n",
    "\n",
    "[Mohammad Khajah, Robert V. Lindsey, Michael Mozer: How Deep is Knowledge Tracing? EDM 2016.](https://www.educationaldatamining.org/EDM2016/proceedings/paper_144.pdf) \n",
    "\n",
    "To capture recency effects, BKT can be extended to allow skill forgetting. Forgetting corresponds to fitting a BKT parameter $F â‰¡ P(K_{s,i+1} = 0 | K_{si} = 1)$, the probability of transitioning from a state of knowing to not knowing a skill. In standard BKT, $F = 0$. Without forgetting, once BKT infers that the student has learned, even a long run of poorly performing trials cannot alter the inferred knowledge state. However, with forgetting, the knowledge state can transition in either direction, which allows the model to be more sensitive to the recent trials: a run of unsuccessful trials is indicative of not knowing the skill. \n",
    "\n",
    "Forgetting is not a new idea to BKT, and in fact was included in the original psychological theory that underlies the notion of binary knowledge state. However, it has not typically been incorporated into BKT. When it has been included in BKT, the motivation was to model forgetting from one day to the next, not forgetting that can occur on a much shorter time scale. Incorporating forgetting can not only sensitize BKT to recent events but can also contextualize trial sequences. Using forgetting, BKT can readily incorporate some information about the absolute trial sequence, and therefore has more potential than classic BKT to be sensitive to interspersed trials in the exercise sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Experimental Pipeline Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an iterator able to user-stratify the interactions typical in a knowledge tracing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_iterator(data):\n",
    "    X = np.arange(len(data.index)) \n",
    "    groups = data['user_id'].values \n",
    "    return GroupKFold(n_splits=10).split(X, groups=groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the iterator to implement a cross validation across the data set {name} with interactions in {data} for a BKT model whose forgetting flag is defined by the {forgets} parameters. At each iteration, each model is evaluated on the Area Under the ROC Curve (AUC) and the Root Mean Squared Error (RMSE). AUC is a measure ranging from 0 to 1, with 0.5 reflecting no ability to distinguish correct from incorrect responses and 1.0 reflecting perfect ability. RMSE is a non-negative error measure, with 0 reflecting the perfect ability to distinguish correct from incorrect responses, and scores higher than 0 reflecting weaknesses in this ability. Both RMSE and AUC are computed by obtaining a prediction on the test set, across all skills, and then using the complete set of predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(name, data, forgets):\n",
    "    ### BEGIN SOLUTION\n",
    "    skills = data['skill_name'].unique()\n",
    "    rmse, auc = [], []\n",
    "    print('Evaluating BKT model with forgets={} on the {} data set with skills={}'.format(forgets, name, skills))\n",
    "    for iteration, (train_index, test_index) in enumerate(create_iterator(data)):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        model = Model(seed=0)\n",
    "        %time model.fit(data=X_train, forgets=forgets) \n",
    "        rmse.append(model.evaluate(data=X_test, metric='rmse'))\n",
    "        auc.append(model.evaluate(data=X_test, metric='auc'))\n",
    "        print('>  Iteration:', iteration, 'RMSE', rmse[-1], 'AUC', auc[-1])\n",
    "    ### END SOLUTION\n",
    "    return {'rmse': rmse, 'auc': auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Overall Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the path where the performance metrics results for each data set will be saved, due to the time-consuming process involving learning a BKT and a BKT+F model. In what follows, we provide you with the code to fit a BKT model, but we will use the already computed results during the lab session due to time constraints - we are more interested in the final discussion.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data set, we run the cross-validation evaluation method, saving AUC and RMSE at each iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "overall_results = {}\n",
    "for name, data in datasets.items():\n",
    "    overall_results[name] = {'BKT': ### ADD YOUR CODE HERE ###, \n",
    "                             'BKT+F': ### ADD YOUR CODE HERE ###} \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then store the results on the disk for future usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(filename, results):\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(results, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('./results/overall_performance.json', overall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be also loaded from an already saved file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(filename):\n",
    "    with open(filename, 'r') as fp:\n",
    "        results = json.load(fp)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results = load_results('./results/overall_performance.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both data sets, we plot the AUC and RMSE scores obtained by the BKT and BKT+F models. What is the impact of forgetting in the final performance? To what extent does this impact change across data sets? What conclusions can we make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overall_results(results):\n",
    "    ### BEGIN SOLUTION\n",
    "    no_datasets = len(results.keys())\n",
    "    \n",
    "    plt.figure(figsize=(12, 4 * no_datasets))\n",
    "    \n",
    "    for d_idx, (dataset, models) in enumerate(results.items()):\n",
    "        \n",
    "        plt.suptitle(dataset)\n",
    "        \n",
    "        plt.subplot(d_idx + 1, 2, 1)\n",
    "        plt.title('AUC')\n",
    "        df_metric = pd.DataFrame(### ADD YOUR CODE HERE ###)\n",
    "        sns.boxplot(x='variable', y='value', data=pd.melt(df_metric))\n",
    "        plt.ylabel('AUC')\n",
    "        plt.xlabel('Model')\n",
    "        \n",
    "        plt.subplot(d_idx + 1, 2, 2)\n",
    "        plt.title('RMSE')\n",
    "        df_metric = pd.DataFrame(### ADD YOUR CODE HERE ###)\n",
    "        sns.boxplot(x='variable', y='value', data=pd.melt(df_metric))\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.xlabel('Model')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_overall_results(overall_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Skill-level Evaluation\n",
    "\n",
    "Unfortunately, the overall performance does not tell us much about the there are discrepancies across skills. To what extent the performance varies across skills and data sets? How does this differ from the results in the overall evaluation? Is there more agreement or less between the different metrics in this and the latter setting? This evaluation can shed light on skills that need further investigation due to lower performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run a similar pipeline. The only change is related to an additional loop along skills. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "skill_results = {}\n",
    "for name, data in datasets.items():\n",
    "    skill_results[name] = {}\n",
    "    for skill in ### ADD YOUR CODE HERE ###:\n",
    "        sdata = ### ADD YOUR CODE HERE ###\n",
    "        skill_results[name][skill] = {'BKT': ### ADD YOUR CODE HERE ###, \n",
    "                                      'BKT+F': ### ADD YOUR CODE HERE ###} \n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('./results/skill_performance.json', skill_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the results are computed, we can load and show them appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_results = load_results('./results/skill_performance.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_skill_results(results):\n",
    "    ### BEGIN SOLUTION\n",
    "    no_datasets = len(results.keys())\n",
    "    \n",
    "    plt.figure(figsize=(20, 10 * no_datasets))\n",
    "    \n",
    "    for d_idx, (dataset, skills) in enumerate(results.items()):\n",
    "        \n",
    "        plt.suptitle(dataset)\n",
    "        \n",
    "        for s_idx, (skill, skill_data) in enumerate(skills.items()):\n",
    "            plt.subplot(2 * no_datasets, len(skills.keys()), s_idx + 1)\n",
    "            plt.title(skill)\n",
    "            df_metric = pd.DataFrame({'BKT': ### ADD YOUR CODE HERE ###, \n",
    "                                      'BKT+F': ### ADD YOUR CODE HERE ###})\n",
    "            sns.boxplot(x='variable', y='value', data=pd.melt(df_metric))\n",
    "            plt.ylabel('AUC')\n",
    "            plt.ylim(0.5, 0.9)\n",
    "            plt.xlabel('Model')\n",
    "        \n",
    "        for s_idx, (skill, skill_data) in enumerate(skills.items()):\n",
    "            plt.subplot(2 * no_datasets, len(skills.keys()), len(skills.keys()) + s_idx + 1)\n",
    "            plt.title(skill)\n",
    "            df_metric = pd.DataFrame({'BKT': ### ADD YOUR CODE HERE ###, \n",
    "                                      'BKT+F': ### ADD YOUR CODE HERE ###})\n",
    "            sns.boxplot(x='variable', y='value', data=pd.melt(df_metric))\n",
    "            plt.ylabel('RMSE')\n",
    "            plt.ylim(0.25, 0.50)\n",
    "            plt.xlabel('Model')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    ### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_skill_results(skill_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning Curve Evaluation\n",
    "\n",
    "Another important aspect that require investigation is the extent to which forgetting impacts on the approximation of the learning curve. How does BKT+F differ from BKT in terms of learning curve? Does BKT+F over or underestimate student's errors? Is the behavior consistent along the opportunities? Plotting the learning curves for the two models will help us answer these questions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_dataset = 'assistments'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first fit and predict on the full data set for a BKT and a BKT+F model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "model = Model(seed=0)\n",
    "%time model.fit(data=data)\n",
    "predictions[sel_dataset] = ### ADD YOUR CODE HERE ###\n",
    "predictions[sel_dataset] = predictions[name].rename({'correct':'y_true', 'correct_predictions': 'y_pred_bkt'}, axis=1)\n",
    "    \n",
    "model = Model(seed=0)\n",
    "%time model.fit(data=data, forgets=True)\n",
    "predictions[sel_dataset]['y_pred_bkt+f'] = ### ADD YOUR CODE HERE ###\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[sel_dataset].to_csv('./results/skill_predictions.json', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to experiment with already computed results, you can directly load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[sel_dataset] = pd.read_csv('./results/skill_predictions.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[sel_dataset].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then re-use the utility functions to plot the learning curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_y_by_x(x, y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    xs = sorted(list(set(x)))\n",
    "    \n",
    "    xv, yv, lcb, ucb, n_obs = [], [], [], [], []\n",
    "    for v in xs:\n",
    "        ys = [y[i] for i, e in enumerate(x) if e == v] \n",
    "        if len(ys) > 0: \n",
    "            xv.append(v) \n",
    "            yv.append(sum(ys) / len(ys))\n",
    "            n_obs.append(len(ys)) \n",
    "        \n",
    "            unique, counts = np.unique(ys, return_counts=True)\n",
    "            counts = dict(zip(unique, counts))\n",
    "\n",
    "            if 0 not in counts:\n",
    "                counts[0] = 0\n",
    "            if 1 not in counts:\n",
    "                counts[1] = 0\n",
    "\n",
    "            ci = sc.stats.beta.interval(0.95, 0.5 + counts[0], 0.5 + counts[1])\n",
    "            lcb.append(ci[0])\n",
    "            ucb.append(ci[1])\n",
    "\n",
    "    return xv, yv, lcb, ucb, n_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(predictions):\n",
    "    for plot_id, skill_name in enumerate(predictions['skill_name'].unique()):\n",
    "\n",
    "        preds = predictions[predictions['skill_name'] == skill_name] \n",
    "\n",
    "        xp = []\n",
    "        yp = {}\n",
    "        for col in preds.columns:\n",
    "            if 'y_' in col:\n",
    "                yp[col] = []\n",
    "\n",
    "        for user_id in preds['user_id'].unique(): \n",
    "            user_preds = preds[preds['user_id'] == user_id] \n",
    "            xp += list(np.arange(len(user_preds)))\n",
    "            for col in preds.columns: \n",
    "                if 'y_' in col: \n",
    "                    yp[col] += user_preds[col].tolist() \n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 2]}) \n",
    "\n",
    "        lines = []\n",
    "        for col in preds.columns:\n",
    "            if 'y_' in col: \n",
    "                x, y, lcb, ucb, n_obs = avg_y_by_x(xp, yp[col])  \n",
    "                y = [1-v for v in y]\n",
    "                if col == 'y_true': \n",
    "                    axs[0].fill_between(x, lcb, ucb, alpha=.1)\n",
    "                model_line, = axs[0].plot(x, y, label=col) \n",
    "                lines.append(model_line) \n",
    "\n",
    "        axs[0].set_title(skill_name)\n",
    "        axs[0].legend(handles=lines)\n",
    "        axs[0].set_ylabel('Error')\n",
    "        axs[0].set_ylim(0, 1)\n",
    "        axs[0].set_xlim(0, None)\n",
    "\n",
    "        axs[1].set_xlabel('#Opportunities')\n",
    "        axs[1].bar([i for i in range(len(n_obs))], n_obs)\n",
    "        axs[1].set_ylabel('#Observations')\n",
    "        axs[1].set_ylim(0, 750)\n",
    "        axs[1].set_xlim(0, None)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the learning curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BKT and DKT\n",
    "\n",
    "Knowledge tracing is one of the key research areas for empowering personalized education. It is a task to model students' mastery level of a skill based on their historical learning trajectories. In recent years, a recurrent neural network model called deep knowledge tracing (DKT) has been proposed to handle the knowledge tracing task and literature has shown that DKT generally outperforms traditional methods.\n",
    "\n",
    "In this lab session, we will create and evaluate DKT models on top of a TensorFlow framework. For those who are not familiar with this framework, we recommended to follow the [official tutorials](https://www.tensorflow.org/tutorials/quickstart/beginner). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give more date to the DKT to learn from, we will use the extended version of the assistments data set.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/assistments.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset.index), len(dataset['skill_name'].unique()), len(dataset['user_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As presented in the lecture, a DKT model is characterized by the following main three components:\n",
    "- **Input**: the one-hot encoded observations at varying time steps. \n",
    "- **Network**: a recurrent neural network that processes the one-hot encoded observations in a time-wise manner. \n",
    "- **Output**: the probabilities for answering skill (or item) correct at the varying time steps.  \n",
    "\n",
    "The first step to enable a DKT experimental pipeline requires to prepare the input and output data to be fed into the model during the training and evaluation phases. TensorFlow has an API, called [TF Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), that supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern: (i) create a source dataset from your input data, (ii) apply dataset transformations to preprocess the data, (iii) iterate over the dataset and process the elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, params):\n",
    "    # Step 1 - Enumerate skill id\n",
    "    df['skill'], skill_codes = pd.factorize(df['skill_name'], sort=True)\n",
    "\n",
    "    # Step 2 - Cross skill id with answer to form a synthetic feature\n",
    "    df['skill_with_answer'] = df['skill'] * 2 + df['correct']\n",
    "\n",
    "    # Step 3 - Convert to a sequence per user id and shift features 1 timestep\n",
    "    seq = df.groupby('user_id').apply(lambda r: (r['skill_with_answer'].values[:-1], r['skill'].values[1:], r['correct'].values[1:],))\n",
    "    \n",
    "    # Step 4 - Get Tensorflow Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(generator=lambda: seq, output_types=(tf.int32, tf.int32, tf.float32))\n",
    "\n",
    "    # Step 5 - Encode categorical features and merge skills with labels to compute target loss.\n",
    "    features_depth = df['skill_with_answer'].max() + 1\n",
    "    skill_depth = df['skill'].max() \n",
    "\n",
    "    dataset = dataset.map(\n",
    "        lambda feat, skill, label: (\n",
    "            tf.one_hot(feat, depth=features_depth),\n",
    "            tf.concat(values=[tf.one_hot(skill, depth=skill_depth), tf.expand_dims(label, -1)], axis=-1)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Step 6 - Pad sequences per batch\n",
    "    dataset = dataset.padded_batch(\n",
    "        batch_size=params['batch_size'],\n",
    "        padding_values=(params['mask_value'], params['mask_value']),\n",
    "        padded_shapes=([None, None], [None, None]),\n",
    "        drop_remainder=True\n",
    "    )\n",
    "\n",
    "    return dataset.repeat(), len(seq), features_depth, skill_depth, skill_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be fed into the model in batches. Therefore, we need to specify in advance how many elements per batch our DKT will receive. Furthermore, all sequences should be of the same length in order to be fed into the model. Given that students have different number of opportunities across skills, we need to define a masking value for those entries that are introduced as a padding into the student's sequences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['batch_size'] = 32\n",
    "params['mask_value'] = -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example of how the TF Dataset API works. We will create the Dataset instance and take 10 examples. To understand how things are going, we will prepare the smaller version of assistments just for this example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset, length, nb_features, nb_skills, skill_codes = prepare_data(datasets['assistments'].copy(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the first 10 batches and pront the shapes. What does each tuple represent? What does each dimension represent for each tuple? Why does the second dimension of each tuple differ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank, (x, y) in enumerate(tf_dataset.take(10)):\n",
    "    print(index, x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the data has been encoded, let's print the integer codes for the skills and the first lines of the original data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(i, v) for i, v in enumerate(skill_codes.tolist()) if v == 'Scatter Plot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets['assistments'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and show the values for the first element of the first batch. How does this representation map to the original dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rank, (i, element) in enumerate(tf_dataset.take(1)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data Split\n",
    "\n",
    "Once our data has been pre-processed, it is time to perform the train-test split - please note that, in this part of the tutorial, we do not adopt a cross-validation evaluation method due to the time constraints. Given that the neural network training requires multiple epochs, we will monitor performance metrics after each epoch on a validation set. Therefore, the split actually includes train-validation-test sets, as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, params):\n",
    "    \n",
    "    def split(dataset, split_size):\n",
    "        split_set = dataset.take(split_size)\n",
    "        dataset = dataset.skip(split_size)\n",
    "        return dataset, split_set\n",
    "\n",
    "    test_size = np.ceil(params['test_ratio'] * params['total_size'])\n",
    "    train_size = ### ADD YOUR CODE HERE ###\n",
    "\n",
    "    train_set, test_set = ### ADD YOUR CODE HERE ###\n",
    "\n",
    "    val_set = None\n",
    "    if params['val_ratio'] > 0:\n",
    "        val_size = ### ADD YOUR CODE HERE ###\n",
    "        train_set, val_set = ### ADD YOUR CODE HERE ###\n",
    "        train_size -= val_size\n",
    "        \n",
    "    return train_set, train_size, test_set, test_size, val_set, val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate our TF Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset, length, nb_features, nb_skills, skill_codes = prepare_data(dataset, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add the total size of the dataset, the test ratio, and the validation ratio to our dictionary of parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['total_size'] = int(length // params['batch_size'])\n",
    "params['test_ratio'] = 0.2\n",
    "params['val_ratio'] = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, train_size, test_set, test_size, val_set, val_size = split_dataset(tf_dataset, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a summary of the data we will fed into the DKT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of students (batches): {}'.format(train_size + val_size + test_size))\n",
    "print('Training set size (batches): {}'.format(train_size))\n",
    "print('Validation set size (batches): {}'.format(val_size))\n",
    "print('Testing set size (batches): {}'.format(test_size))\n",
    "print('Number of skills (batches): {}'.format(nb_skills))\n",
    "print('Number of features in the input (batches): {}'.format(nb_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['verbose'] = 1 # Verbose = {0,1,2}\n",
    "params['best_model_weights'] = 'weights/bestmodel' # File to save the model\n",
    "params['optimizer'] = 'adam' # Optimizer to use\n",
    "params['backbone_nn'] = tf.keras.layers.LSTM # Backbone neural network\n",
    "params['recurrent_units'] = 64 # Number of LSTM units\n",
    "params['epochs'] = 10 # Number of epochs to train\n",
    "params['dropout_rate'] = 0.3 # Dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering that we padded the sequences such that all have the same length, we need to remove predictions on the time step associated with padding. To this end, we implement a function calle get_target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target(y_true, y_pred, mask_value=params['mask_value']):\n",
    "    \n",
    "    # Get skills and labels from y_true\n",
    "    mask = 1. - tf.cast(tf.equal(y_true, mask_value), y_true.dtype)\n",
    "    y_true = y_true * mask\n",
    "\n",
    "    skills, y_true = tf.split(y_true, num_or_size_splits=[-1, 1], axis=-1)\n",
    "\n",
    "    # Get predictions for each skill\n",
    "    y_pred = tf.reduce_sum(y_pred * skills, axis=-1, keepdims=True)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While training and evaluating the model, we will monitor the following performance metrics: binary accuracy, AUC, precision, and recall. Please, note that we need to process our targets before using the default TensorFlow metric functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAccuracy(tf.keras.metrics.BinaryAccuracy):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(BinaryAccuracy, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "class AUC(tf.keras.metrics.AUC):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(AUC, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "class Precision(tf.keras.metrics.Precision):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(Precision, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)\n",
    "\n",
    "class Recall(tf.keras.metrics.Recall):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        true, pred = get_target(y_true, y_pred)\n",
    "        super(Recall, self).update_state(y_true=true, y_pred=pred, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same discourse for the loss. As we have seen in the lecture, we need to use a binary crossentropy function to compute the loss the model makes in predicting the labels. Please, note that we need to use the get_target function in this case too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomBinaryCrossEntropy(y_true, y_pred):    \n",
    "    y_true, y_pred = get_target(y_true, y_pred)\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the DKT architecture using the TensorFlow functional API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(nb_features, nb_skills, params):\n",
    "    \n",
    "    # Create the model architecture\n",
    "    inputs = tf.keras.Input(shape=(None, nb_features), name='inputs')\n",
    "    x = tf.keras.layers.Masking(mask_value=params['mask_value'])(inputs)\n",
    "    x = params['backbone_nn'](params['recurrent_units'], return_sequences=True, dropout=params['dropout_rate'])(x)\n",
    "    dense = tf.keras.layers.Dense(nb_skills, activation='sigmoid')\n",
    "    outputs = tf.keras.layers.TimeDistributed(dense, name='outputs')(x)\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=outputs, name='DKT')\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=CustomBinaryCrossEntropy, \n",
    "                  optimizer=params['optimizer'], \n",
    "                  metrics=[BinaryAccuracy(), AUC(), Precision(), Recall()])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(nb_features, nb_skills, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we show a table summary of the DKT model architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model Fitting\n",
    "\n",
    "The data and the model have been prepared and now we can proceed with the model fitting stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckp_callback = tf.keras.callbacks.ModelCheckpoint(params['best_model_weights'], save_best_only=True, save_weights_only=True)\n",
    "\n",
    "history = model.fit(train_set, epochs=params['epochs'], steps_per_epoch=train_size, validation_data=val_set, callbacks=[ckp_callback], verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better understanding of our the fitting process went, we can plot each score along epochs. How does the fitting process influence the loss? How is this reflected to the performance metrics? What conclusions could we made? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/dkt_history.npy', hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.load('./results/dkt_history.npy', allow_pickle=True)[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "plt.figure(figsize=(20, 8))\n",
    "    \n",
    "plt_idx = 1    \n",
    "for name in hist.keys():\n",
    "    if not 'val' in name:\n",
    "        plt.subplot(2, 3, plt_idx)\n",
    "        plt.plot(### ADD YOUR CODE HERE ###)\n",
    "        plt.plot(### ADD YOUR CODE HERE ###)\n",
    "        plt.ylabel(name)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc='upper left')\n",
    "        plt_idx += 1\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Evaluation\n",
    "\n",
    "Using the DKT model trained in the previous step, visually represent the AUC score obtained on the test set. What can we say about model performance? Were students' answer correctly classified? How are the results different from those of BKT and BKT+F?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we restore the model that has achieved the highest performance in the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(params['best_model_weights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call the evaluate method to compute all performance metrics set up at compiling time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.evaluate(test_set, verbose=params['verbose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the auc score in a variable for future re-use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dkt = ### ADD YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the BKT and BKT+F baselines on the same train-test split\n",
    "\n",
    "Please, remember that the comparison should be made on the same train and test sets. Therefore, we need to split the original data set according to what we have done for DKT.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we identify the user ids associated to the train, validation, and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "users = dataset['user_id'].unique()\n",
    "test_users = users[:int(test_size * params['batch_size'])]\n",
    "val_users = users[int(test_size * params['batch_size']):int((test_size+val_size) * params['batch_size'])]\n",
    "train_users = users[int((test_size+val_size) * params['batch_size']):]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_users), len(val_users), len(train_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the original data set according to the train, validation, and test users.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "bkt_train_set = dataset[dataset['user_id'].isin(train_users)]\n",
    "bkt_val_set = dataset[dataset['user_id'].isin(val_users)]\n",
    "bkt_test_set = dataset[dataset['user_id'].isin(test_users)]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit and evaluate a traditional BKT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "model = Model(seed=0)\n",
    "%time model.fit(data=bkt_train_set) \n",
    "auc_bkt = ### ADD YOUR CODE HERE ###\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and a BKT model with forgetting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "model = Model(seed=0)\n",
    "%time ### ADD YOUR CODE HERE ###\n",
    "auck_bktf = ### ADD YOUR CODE HERE ###\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/dkt_comparison.npy', [auc_dkt, auc_bkt, auc_bktf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_dkt, auc_bkt, auc_bktf = np.load('./results/dkt_comparison.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of the performance metrics across models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "labels = ['DKT', 'BKT', 'BKT+F']\n",
    "scores = [auc_dkt, auc_bkt, auc_bktf]\n",
    "\n",
    "x_pos = np.arange(len(labels))\n",
    "plt.bar(### ADD YOUR CODE HERE ###)\n",
    "\n",
    "plt.xticks(x_pos, labels)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim(.65, .85)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Ablation Study\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backbone Network\n",
    "As seen during the lecture, the recurrent neural network used as a backbone for the DKT model is represented by an LSTM. To what extent an LSTM leads to higher performance than other recurrent networks such as RNNs and GRUs? How does it change across data sets? Please, do not forget that the data and the patterns behind it may lead to different rankings among backbone networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_nn_range = [tf.keras.layers.GRU, tf.keras.layers.LSTM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "aucs_dkt = []\n",
    "for backbone_nn in backbone_nn_range: \n",
    "    print('Backbone NN', backbone_nn)\n",
    "    params['backbone_nn'] = backbone_nn\n",
    "    model = ### ADD YOUR CODE HERE ###\n",
    "    model.fit(train_set, epochs=params['epochs'], steps_per_epoch=train_size, validation_data=val_set, callbacks=[ckp_callback], verbose=1)\n",
    "    aucs_dkt.append(### ADD YOUR CODE HERE ###)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/backbone_nn_range.npy', aucs_dkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_dkt = np.load('./results/backbone_nn_range.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "labels = ### ADD YOUR CODE HERE ###\n",
    "scores = ### ADD YOUR CODE HERE ###\n",
    "\n",
    "x_pos = np.arange(len(labels))\n",
    "plt.bar(x_pos, scores, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "\n",
    "plt.xticks(x_pos, labels)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim(.80, .83)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Units\n",
    "As seen during the lecture, sometimes we are interested in playing with the hyper-parameters. How do the results vary when changing the number of units for the LSTM? Which number of units makes more sense? What happens when the number of units is very small? What happens when the number of units is very large? These questions are all fundamental to deeply understand a BKT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recurrent_units_range = [8, 16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "aucs_dkt = []\n",
    "for recurrent_units in recurrent_units_range: \n",
    "    print('Recurrent Units', recurrent_units)\n",
    "    params['recurrent_units'] = ### ADD YOUR CODE HERE ###\n",
    "    model = create_model(nb_features, nb_skills, params)\n",
    "    model.fit(train_set, epochs=params['epochs'], steps_per_epoch=train_size, validation_data=val_set, callbacks=[ckp_callback], verbose=1)\n",
    "    aucs_dkt.append(### ADD YOUR CODE HERE ###)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./results/recurrent_units_range.npy', aucs_dkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_dkt = np.load('./results/recurrent_units_range.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "labels = ### ADD YOUR CODE HERE ###\n",
    "scores = ### ADD YOUR CODE HERE ###\n",
    "\n",
    "x_pos = np.arange(len(labels))\n",
    "plt.bar(x_pos, scores, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "\n",
    "plt.xticks(x_pos, labels)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim(.75, .85)\n",
    "\n",
    "plt.show()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "In this tutorial, you experimented with two advanced knowledge tracing strategies, namely BKT+F and DKT. With these advanced modelling, you built and evaluated the performance of student models in correctly classifying students' answers to problems pertaining to certain skills, going beyond a couple of limitations showed by the traditional BKT. Particular focus was given to the discussion and interpretation of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework05_KnowledgeTracing-Example-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
